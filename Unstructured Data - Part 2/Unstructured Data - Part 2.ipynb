{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unstructured Data - Continued\n",
    "\n",
    "Picking up where we left off - in this lab we'll use the same book data set that we web scraped. We'll pick a large genre, and see if we can identify some common words for the genre. We'll then see if we can distinguish genre's from each another.\n",
    "\n",
    "This brings us to the fundamental problem of both data engineering and data science work with unstructured data. That is, in general, computers understand numbers better than words. This sounds obvious, but it is the fundamental problem that has to be solved when extracting information from unstructured data.\n",
    "\n",
    "This lab will require a bit of handholding, as there are some complex concepts we're going jump into.\n",
    "\n",
    "Let's get started. . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas, nltk, from nltk.tokenize.api import TokenizerI, matplotlib (inline), numpy\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.api import TokenizerI\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#import the model_selection class from scikit\n",
    "\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Price</th>\n",
       "      <th>Description</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Link</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Light in the Attic</td>\n",
       "      <td>Ã‚Â£51.77</td>\n",
       "      <td>It's hard to imagine a world without A Light i...</td>\n",
       "      <td>Three</td>\n",
       "      <td>a-light-in-the-attic_1000/index.html</td>\n",
       "      <td>Poetry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tipping the Velvet</td>\n",
       "      <td>Ã‚Â£53.74</td>\n",
       "      <td>\"Erotic and absorbing...Written with starling ...</td>\n",
       "      <td>One</td>\n",
       "      <td>tipping-the-velvet_999/index.html</td>\n",
       "      <td>Historical Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Soumission</td>\n",
       "      <td>Ã‚Â£50.10</td>\n",
       "      <td>Dans une France assez proche de la nÃƒÂ´tre, un ...</td>\n",
       "      <td>One</td>\n",
       "      <td>soumission_998/index.html</td>\n",
       "      <td>Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sharp Objects</td>\n",
       "      <td>Ã‚Â£47.82</td>\n",
       "      <td>WICKED above her hipbone, GIRL across her hear...</td>\n",
       "      <td>Four</td>\n",
       "      <td>sharp-objects_997/index.html</td>\n",
       "      <td>Mystery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sapiens: A Brief History of Humankind</td>\n",
       "      <td>Ã‚Â£54.23</td>\n",
       "      <td>From a renowned historian comes a groundbreaki...</td>\n",
       "      <td>Five</td>\n",
       "      <td>sapiens-a-brief-history-of-humankind_996/index...</td>\n",
       "      <td>History</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Title    Price  \\\n",
       "0                   A Light in the Attic  Ã‚Â£51.77   \n",
       "1                     Tipping the Velvet  Ã‚Â£53.74   \n",
       "2                             Soumission  Ã‚Â£50.10   \n",
       "3                          Sharp Objects  Ã‚Â£47.82   \n",
       "4  Sapiens: A Brief History of Humankind  Ã‚Â£54.23   \n",
       "\n",
       "                                         Description Rating  \\\n",
       "0  It's hard to imagine a world without A Light i...  Three   \n",
       "1  \"Erotic and absorbing...Written with starling ...    One   \n",
       "2  Dans une France assez proche de la nÃƒÂ´tre, un ...    One   \n",
       "3  WICKED above her hipbone, GIRL across her hear...   Four   \n",
       "4  From a renowned historian comes a groundbreaki...   Five   \n",
       "\n",
       "                                                Link               Genre  \n",
       "0               a-light-in-the-attic_1000/index.html              Poetry  \n",
       "1                  tipping-the-velvet_999/index.html  Historical Fiction  \n",
       "2                          soumission_998/index.html             Fiction  \n",
       "3                       sharp-objects_997/index.html             Mystery  \n",
       "4  sapiens-a-brief-history-of-humankind_996/index...             History  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read in the book data\n",
    "\n",
    "data = pd.read_csv('scraped_books.csv', index_col=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Default           152\n",
       "Nonfiction        110\n",
       "Sequential Art     75\n",
       "Add a comment      67\n",
       "Fiction            65\n",
       "Young Adult        54\n",
       "Fantasy            48\n",
       "Romance            35\n",
       "Mystery            32\n",
       "Food and Drink     30\n",
       "Name: Genre, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#display the top 10 genres\n",
    "\n",
    "data.Genre.value_counts()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the top few genres are fairly broad, so let's pick a genre that likely has more descriptive keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Price</th>\n",
       "      <th>Description</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Link</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Unicorn Tracks</td>\n",
       "      <td>Ã‚Â£18.78</td>\n",
       "      <td>After a savage attack drives her from her home...</td>\n",
       "      <td>Three</td>\n",
       "      <td>unicorn-tracks_951/index.html</td>\n",
       "      <td>Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Saga, Volume 6 (Saga (Collected Editions) #6)</td>\n",
       "      <td>Ã‚Â£25.02</td>\n",
       "      <td>After a dramatic time jump, the three-time Eis...</td>\n",
       "      <td>Three</td>\n",
       "      <td>saga-volume-6-saga-collected-editions-6_924/in...</td>\n",
       "      <td>Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Princess Between Worlds (Wide-Awake Princess #5)</td>\n",
       "      <td>Ã‚Â£13.34</td>\n",
       "      <td>Just as Annie and Liam are busy making plans t...</td>\n",
       "      <td>Five</td>\n",
       "      <td>princess-between-worlds-wide-awake-princess-5_...</td>\n",
       "      <td>Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Masks and Shadows</td>\n",
       "      <td>Ã‚Â£56.40</td>\n",
       "      <td>The year is 1779, and Carlo Morelli, the most ...</td>\n",
       "      <td>Two</td>\n",
       "      <td>masks-and-shadows_909/index.html</td>\n",
       "      <td>Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>Crown of Midnight (Throne of Glass #2)</td>\n",
       "      <td>Ã‚Â£43.29</td>\n",
       "      <td>\"A line that should never be crossed is about ...</td>\n",
       "      <td>Three</td>\n",
       "      <td>crown-of-midnight-throne-of-glass-2_888/index....</td>\n",
       "      <td>Fantasy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title    Price  \\\n",
       "49                                     Unicorn Tracks  Ã‚Â£18.78   \n",
       "76      Saga, Volume 6 (Saga (Collected Editions) #6)  Ã‚Â£25.02   \n",
       "81   Princess Between Worlds (Wide-Awake Princess #5)  Ã‚Â£13.34   \n",
       "91                                  Masks and Shadows  Ã‚Â£56.40   \n",
       "112            Crown of Midnight (Throne of Glass #2)  Ã‚Â£43.29   \n",
       "\n",
       "                                           Description Rating  \\\n",
       "49   After a savage attack drives her from her home...  Three   \n",
       "76   After a dramatic time jump, the three-time Eis...  Three   \n",
       "81   Just as Annie and Liam are busy making plans t...   Five   \n",
       "91   The year is 1779, and Carlo Morelli, the most ...    Two   \n",
       "112  \"A line that should never be crossed is about ...  Three   \n",
       "\n",
       "                                                  Link    Genre  \n",
       "49                       unicorn-tracks_951/index.html  Fantasy  \n",
       "76   saga-volume-6-saga-collected-editions-6_924/in...  Fantasy  \n",
       "81   princess-between-worlds-wide-awake-princess-5_...  Fantasy  \n",
       "91                    masks-and-shadows_909/index.html  Fantasy  \n",
       "112  crown-of-midnight-throne-of-glass-2_888/index....  Fantasy  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a slice of the dataframe that is only books in the Fantasy genre and show the first 5 rows of it\n",
    "\n",
    "fantasy = data[data['Genre'] == 'Fantasy']\n",
    "fantasy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dipping our toes in. . .\n",
    "\n",
    "Now let's split this into a training and test set. The idea here is that we want to find common words in 70% of the Fantasy book descriptions. Then we'll see if we can accurately predict the other 30% of the books. There is a very easy way of doing this using scikit-learn's [*sklearn.model_selection.train_test_split()*](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function. To be totally clear, this is way overkill for this application, but it's a cool way to start using scikit, so let's do it. ðŸ˜‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the dataframe into a train (70%) and test (30%) set\n",
    "\n",
    "train, test = model_selection.train_test_split(fantasy, test_size=0.3, train_size=0.7, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the len of your train set to confirm you split properly\n",
    "\n",
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the len of your test set to confirm you split properly\n",
    "\n",
    "len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create a tokenization function that you can use apply to each row. It should tokenize the given row's description, remove stopwords, create a FreqDist for the tokens, remove anything that is < 1 character, and then return the top 5 most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import stopwords and set them to use the english subset\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the tokenization function \n",
    "\n",
    "def common_word_getter(row):\n",
    "    words = nltk.word_tokenize(row.Description)\n",
    "    frequency = nltk.FreqDist(words)\n",
    "    frequency = [(w, f) for (w, f) in frequency.items() if w.lower() not in stopwords]\n",
    "    frequency = [(w, f) for (w, f) in frequency if len(w) > 1]\n",
    "    frequency.sort(key=lambda tup: tup[1], reverse=True)\n",
    "    most_common = frequency[:5]\n",
    "    return most_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list of the most common words per book by iterating through the training set and applying your function\n",
    "\n",
    "common_list = []\n",
    "\n",
    "for index, row in train.iterrows():\n",
    "    common_list.extend([i[0] for i in common_word_getter(row)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['since',\n",
       " 'months',\n",
       " 'London',\n",
       " 'four',\n",
       " 'stone',\n",
       " 'Peter',\n",
       " \"'s\",\n",
       " 'Probationary',\n",
       " 'Constable',\n",
       " 'Grant']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show the top of the common list\n",
    "\n",
    "common_list[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annnnd now we jump straight into the deep end ðŸ¤¯\n",
    "\n",
    "Up until this point, we've tokenized bits of text manually, and manually sorted, removed stopwords, etc. The hope here is to build some intuition around the steps required to work with text. Now, we're going to introduce some industry standard tools that do many of these steps together. We'll still go step by step, but these tools will allow us to abstract from some of the 'manual-ness' we've experienced thus far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "#from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#instantiate an instance of the CountVectorizer() class\n",
    "\n",
    "vect = CountVectorizer()\n",
    "vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class is really cool. We'll run it on our Description column, and use it to vectorize each piece of text. The vectorizing here is extremely simple, and is the most basic way of making the 'words to numbers' jump we discussed above.\n",
    "\n",
    "Essentially, you take every word in the piece of text you're analyzing and replace it with a 1. Then, for each additional instance of the same word, you add 1. We'll go step by step to show what's happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(max_features=10,\n",
       "                stop_words=[&#x27;i&#x27;, &#x27;me&#x27;, &#x27;my&#x27;, &#x27;myself&#x27;, &#x27;we&#x27;, &#x27;our&#x27;, &#x27;ours&#x27;,\n",
       "                            &#x27;ourselves&#x27;, &#x27;you&#x27;, &quot;you&#x27;re&quot;, &quot;you&#x27;ve&quot;, &quot;you&#x27;ll&quot;,\n",
       "                            &quot;you&#x27;d&quot;, &#x27;your&#x27;, &#x27;yours&#x27;, &#x27;yourself&#x27;, &#x27;yourselves&#x27;,\n",
       "                            &#x27;he&#x27;, &#x27;him&#x27;, &#x27;his&#x27;, &#x27;himself&#x27;, &#x27;she&#x27;, &quot;she&#x27;s&quot;,\n",
       "                            &#x27;her&#x27;, &#x27;hers&#x27;, &#x27;herself&#x27;, &#x27;it&#x27;, &quot;it&#x27;s&quot;, &#x27;its&#x27;,\n",
       "                            &#x27;itself&#x27;, ...])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(max_features=10,\n",
       "                stop_words=[&#x27;i&#x27;, &#x27;me&#x27;, &#x27;my&#x27;, &#x27;myself&#x27;, &#x27;we&#x27;, &#x27;our&#x27;, &#x27;ours&#x27;,\n",
       "                            &#x27;ourselves&#x27;, &#x27;you&#x27;, &quot;you&#x27;re&quot;, &quot;you&#x27;ve&quot;, &quot;you&#x27;ll&quot;,\n",
       "                            &quot;you&#x27;d&quot;, &#x27;your&#x27;, &#x27;yours&#x27;, &#x27;yourself&#x27;, &#x27;yourselves&#x27;,\n",
       "                            &#x27;he&#x27;, &#x27;him&#x27;, &#x27;his&#x27;, &#x27;himself&#x27;, &#x27;she&#x27;, &quot;she&#x27;s&quot;,\n",
       "                            &#x27;her&#x27;, &#x27;hers&#x27;, &#x27;herself&#x27;, &#x27;it&#x27;, &quot;it&#x27;s&quot;, &#x27;its&#x27;,\n",
       "                            &#x27;itself&#x27;, ...])</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer(max_features=10,\n",
       "                stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                            'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                            \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                            'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                            'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                            'itself', ...])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set the stop_words argument equal to the stopwords we defined before, and set max_features to 10\n",
    "\n",
    "vect = CountVectorizer(stop_words=stopwords, max_features=10)\n",
    "vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<33x10 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 107 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use vect.fit_transform() on the Description column in the training set to find the vectors of the training data\n",
    "\n",
    "train_vectors = vect.fit_transform(train.Description)\n",
    "train_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/jupyterlab/3.4.3/libexec/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['book',\n",
       " 'find',\n",
       " 'four',\n",
       " 'life',\n",
       " 'new',\n",
       " 'one',\n",
       " 'power',\n",
       " 'series',\n",
       " 'seven',\n",
       " 'world']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the feature names that sklearn found\n",
    "\n",
    "vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting vector is what's called a 'sparse matrix'. This is a numpy data type for storing large, sparse arrays. The dimensions of the vector are:\n",
    "\n",
    "- rows = # of samples vectorized\n",
    "- columns = # of features\n",
    "\n",
    "Now - here's the really mindblowing part :)\n",
    "\n",
    "Now that we've defined this 'vect' class, it will 'remember' it's vocabulary the next time we call it. This is because of the object oriented concept called inheritance. [This](https://towardsdatascience.com/custom-transformers-and-ml-data-pipelines-with-python-20ea2a7adb65) Medium post does a reasonable job at explaining the concept (as applies to sklearn specifically). \n",
    "\n",
    "For our purposes, it means that once we've instantiated the class on the training set, we can call it on the test set and it will remember the common words from the training set. This can be a bit confusing, so definitely do some reading on this before moving on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<15x10 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 39 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#repeat the above on the test set\n",
    "\n",
    "test_vectors = vect.transform(test.Description)\n",
    "test_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So now we have two matrices:\n",
    "\n",
    "- The train vector has the vectorization of the ten most common words throughout the corpus\n",
    "- The test vector has the vectorization of each of those ten words in the test set \n",
    "\n",
    "Now, if our goal is to use these training vectors to predict the genre of our test set, we need some way of comparing these vectors against each other. There are many ways to do this, so we'll start with using the cosine similarity. If you're really interested in understanding what's going on under the hood, read up on it [here.](https://en.wikipedia.org/wiki/Cosine_similarity)\n",
    "\n",
    "At a high level, what we need to do a few things:\n",
    "\n",
    "- First, we need to find some measure of 'averageness' across our training vectors. This wil be a single vector that represents the average presence of each term across the training corpus.\n",
    "- Second, we need to score each of the test vectors against this 'average' vector\n",
    "- Third, we should look at those scores and see if there is any discernible pattern in them\n",
    "- Finally, the real test of 'predictiveness' will be to shuffle in some other genres with out test set, score them all against the average vector, and see if we can accurately distinguish the fantasy books from the rest of the data.\n",
    "\n",
    "Let's go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.75757576, 0.57575758, 0.54545455, 0.6969697 , 1.12121212,\n",
       "         0.96969697, 0.57575758, 0.90909091, 0.60606061, 0.93939394]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find the average vector in the training set\n",
    "\n",
    "average_vector = train_vectors.mean(axis=0)\n",
    "average_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/jupyterlab/3.4.3/libexec/lib/python3.10/site-packages/sklearn/utils/validation.py:727: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#score the test vectors using their cosine distance from the average vector\n",
    "\n",
    "scores = cosine_distances(X=average_vector, Y=test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.53433771, 0.34467012, 0.7705006 , 0.52428744, 0.398394  ,\n",
       "        0.41920634, 0.6134747 , 0.6134747 , 0.24644118, 0.22091279,\n",
       "        0.56605469, 0.62555361, 0.48753501, 0.6134747 , 0.54159191]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show the scores\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#histogram the scores\n",
    "\n",
    "plt.hist(scores[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rw/1rnjc7b90vd_mr3753j04_s40000gp/T/ipykernel_59679/1967842870.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  out_of_genre = test.append(data[data['Genre']!='Fantasy'].sample(50))\n"
     ]
    }
   ],
   "source": [
    "#randomly shuffle in 50 rows from the other genres to the test set, and re-run the same prediction steps we ran above\n",
    "#feel free to also exclude the 'Add a comment' and 'Default' genres, as we don't know what their true genre is\n",
    "\n",
    "out_of_genre = test.append(data[data['Genre']!='Fantasy'].sample(50))\n",
    "out_of_genre = out_of_genre[out_of_genre['Genre']!='Default']\n",
    "out_of_genre = out_of_genre[out_of_genre['Genre']!='Add a comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<52x10 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 129 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run the shuffled data through the vect class\n",
    "\n",
    "out_of_genre_vects = vect.transform(out_of_genre.Description)\n",
    "out_of_genre_vects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/jupyterlab/3.4.3/libexec/lib/python3.10/site-packages/sklearn/utils/validation.py:727: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#score these out of genre vectors against the average vector we created above\n",
    "\n",
    "out_of_genre_scores = cosine_distances(average_vector, out_of_genre_vects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.53433771, 0.34467012, 0.7705006 , 0.52428744, 0.398394  ,\n",
       "        0.41920634, 0.6134747 , 0.6134747 , 0.24644118, 0.22091279,\n",
       "        0.56605469, 0.62555361, 0.48753501, 0.6134747 , 0.54159191,\n",
       "        0.48865924, 0.59210383, 0.50344049, 1.        , 0.47267787,\n",
       "        0.62727198, 1.        , 0.65494089, 1.        , 0.50302931,\n",
       "        0.27951574, 0.40236504, 0.54570857, 0.28170218, 0.51183643,\n",
       "        0.46521632, 0.41811879, 0.32326994, 0.59002801, 0.54545707,\n",
       "        0.55704786, 0.62555361, 0.42740334, 1.        , 0.33480641,\n",
       "        0.33873941, 0.37312871, 0.40025619, 0.2988144 , 0.42774743,\n",
       "        0.55308012, 0.4645014 , 1.        , 0.52169934, 0.38630866,\n",
       "        0.39066061, 1.        ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show the out of genre scores\n",
    "\n",
    "out_of_genre_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#histogram the out of genre scores\n",
    "\n",
    "plt.hist(out_of_genre_scores[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From just looking at the histogram, things don't look great. The out of sample score distribution looks a kind of similar to the test score distribution so it may be harder to distinguish the Fantasy genre than we thought.\n",
    "\n",
    "Let's check by adding these scores back into the out of sample dataframe we created, and taking a look at the high scoring rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the scores as a 'score' column to the out_of_sample df\n",
    "\n",
    "out_of_genre['score'] = out_of_genre_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort the df by your score column, from highest value to lowest\n",
    "\n",
    "try_it_out = out_of_genre.sort_values(by='score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Price</th>\n",
       "      <th>Description</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Link</th>\n",
       "      <th>Genre</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Alice in Wonderland (Alice's Adventures in Won...</td>\n",
       "      <td>Ã‚Â£55.53</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n</td>\n",
       "      <td>One</td>\n",
       "      <td>alice-in-wonderland-alices-adventures-in-wonde...</td>\n",
       "      <td>Classics</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948</th>\n",
       "      <td>Shameless</td>\n",
       "      <td>Ã‚Â£58.35</td>\n",
       "      <td>***USA TODAY BESTSELLER***BradyÃ¢Â€Â¦What the hel...</td>\n",
       "      <td>Three</td>\n",
       "      <td>shameless_52/index.html</td>\n",
       "      <td>New Adult</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>New Moon (Twilight #2)</td>\n",
       "      <td>Ã‚Â£12.86</td>\n",
       "      <td>Also see: Alternate Cover Editions for this IS...</td>\n",
       "      <td>Four</td>\n",
       "      <td>new-moon-twilight-2_105/index.html</td>\n",
       "      <td>Young Adult</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>Fruits Basket, Vol. 4 (Fruits Basket #4)</td>\n",
       "      <td>Ã‚Â£50.44</td>\n",
       "      <td>When the infamous Akito makes an in-class appe...</td>\n",
       "      <td>Four</td>\n",
       "      <td>fruits-basket-vol-4-fruits-basket-4_321/index....</td>\n",
       "      <td>Sequential Art</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>Killing Floor (Jack Reacher #1)</td>\n",
       "      <td>Ã‚Â£31.49</td>\n",
       "      <td>Ex-military policeman Jack Reacher is a drifte...</td>\n",
       "      <td>Four</td>\n",
       "      <td>killing-floor-jack-reacher-1_382/index.html</td>\n",
       "      <td>Thriller</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>What's It Like in Space?: Stories from Astrona...</td>\n",
       "      <td>Ã‚Â£19.60</td>\n",
       "      <td>Everyone wonders what it's really like in spac...</td>\n",
       "      <td>Two</td>\n",
       "      <td>whats-it-like-in-space-stories-from-astronauts...</td>\n",
       "      <td>Nonfiction</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>The Mirror &amp; the Maze (The Wrath and the Dawn ...</td>\n",
       "      <td>Ã‚Â£29.38</td>\n",
       "      <td>The city of Rey is burning. With smoke billowi...</td>\n",
       "      <td>One</td>\n",
       "      <td>the-mirror-the-maze-the-wrath-and-the-dawn-15_...</td>\n",
       "      <td>Fantasy</td>\n",
       "      <td>0.770501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>Digital Fortress</td>\n",
       "      <td>Ã‚Â£58.00</td>\n",
       "      <td>Before the multi-million, runaway bestseller T...</td>\n",
       "      <td>Five</td>\n",
       "      <td>digital-fortress_369/index.html</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>0.654941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>Booked</td>\n",
       "      <td>Ã‚Â£17.49</td>\n",
       "      <td>Like lightning/you strike/fast and free/legs z...</td>\n",
       "      <td>Five</td>\n",
       "      <td>booked_365/index.html</td>\n",
       "      <td>Poetry</td>\n",
       "      <td>0.627272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Princess Between Worlds (Wide-Awake Princess #5)</td>\n",
       "      <td>Ã‚Â£13.34</td>\n",
       "      <td>Just as Annie and Liam are busy making plans t...</td>\n",
       "      <td>Five</td>\n",
       "      <td>princess-between-worlds-wide-awake-princess-5_...</td>\n",
       "      <td>Fantasy</td>\n",
       "      <td>0.625554</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Title    Price  \\\n",
       "995  Alice in Wonderland (Alice's Adventures in Won...  Ã‚Â£55.53   \n",
       "948                                          Shameless  Ã‚Â£58.35   \n",
       "895                             New Moon (Twilight #2)  Ã‚Â£12.86   \n",
       "679           Fruits Basket, Vol. 4 (Fruits Basket #4)  Ã‚Â£50.44   \n",
       "618                    Killing Floor (Jack Reacher #1)  Ã‚Â£31.49   \n",
       "221  What's It Like in Space?: Stories from Astrona...  Ã‚Â£19.60   \n",
       "927  The Mirror & the Maze (The Wrath and the Dawn ...  Ã‚Â£29.38   \n",
       "631                                   Digital Fortress  Ã‚Â£58.00   \n",
       "635                                             Booked  Ã‚Â£17.49   \n",
       "81    Princess Between Worlds (Wide-Awake Princess #5)  Ã‚Â£13.34   \n",
       "\n",
       "                                           Description Rating  \\\n",
       "995                                       \\n\\n\\n\\n\\n\\n    One   \n",
       "948  ***USA TODAY BESTSELLER***BradyÃ¢Â€Â¦What the hel...  Three   \n",
       "895  Also see: Alternate Cover Editions for this IS...   Four   \n",
       "679  When the infamous Akito makes an in-class appe...   Four   \n",
       "618  Ex-military policeman Jack Reacher is a drifte...   Four   \n",
       "221  Everyone wonders what it's really like in spac...    Two   \n",
       "927  The city of Rey is burning. With smoke billowi...    One   \n",
       "631  Before the multi-million, runaway bestseller T...   Five   \n",
       "635  Like lightning/you strike/fast and free/legs z...   Five   \n",
       "81   Just as Annie and Liam are busy making plans t...   Five   \n",
       "\n",
       "                                                  Link           Genre  \\\n",
       "995  alice-in-wonderland-alices-adventures-in-wonde...        Classics   \n",
       "948                            shameless_52/index.html       New Adult   \n",
       "895                 new-moon-twilight-2_105/index.html     Young Adult   \n",
       "679  fruits-basket-vol-4-fruits-basket-4_321/index....  Sequential Art   \n",
       "618        killing-floor-jack-reacher-1_382/index.html        Thriller   \n",
       "221  whats-it-like-in-space-stories-from-astronauts...      Nonfiction   \n",
       "927  the-mirror-the-maze-the-wrath-and-the-dawn-15_...         Fantasy   \n",
       "631                    digital-fortress_369/index.html         Fiction   \n",
       "635                              booked_365/index.html          Poetry   \n",
       "81   princess-between-worlds-wide-awake-princess-5_...         Fantasy   \n",
       "\n",
       "        score  \n",
       "995  1.000000  \n",
       "948  1.000000  \n",
       "895  1.000000  \n",
       "679  1.000000  \n",
       "618  1.000000  \n",
       "221  1.000000  \n",
       "927  0.770501  \n",
       "631  0.654941  \n",
       "635  0.627272  \n",
       "81   0.625554  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show the first 10 rows of the df\n",
    "\n",
    "try_it_out.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fantasy           6\n",
       "Nonfiction        3\n",
       "Classics          1\n",
       "New Adult         1\n",
       "Young Adult       1\n",
       "Sequential Art    1\n",
       "Thriller          1\n",
       "Fiction           1\n",
       "Poetry            1\n",
       "Science           1\n",
       "Psychology        1\n",
       "Food and Drink    1\n",
       "Art               1\n",
       "Name: Genre, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show the Genre value counts of the 20 highest scoring books\n",
    "\n",
    "try_it_out.head(20).Genre.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lessons learned:\n",
    "\n",
    "If you look at the dataframe above, you'll see our scoring method performed very poorly. This is for a number of reasons:\n",
    "\n",
    "- The vectorization we used is extremely simplistic and just uses word counts. In a real world application, a data scientist would use a far more advanced embedding method.\n",
    "- Our dataset is fairly small, and it seems like the Fantasy genre doesn't have many unique words like we'd hoped.\n",
    "    - We can tell this by looking at the top 10 tokens in the training set. They don't seem particularly 'fantasy-like' to me:\n",
    "        - ['book', 'find', 'four', 'life', 'new', 'one', 'power', 'series', 'seven', 'world']\n",
    "- There's clearly a high amount of linguistic overlap between Fantasy, Non-Fiction, Young Adult, and Sequential Art book descriptions. You can tell this by looking at the highest scoring books in our out of sample test we ran. 14 out of the top 20 scoring books fall into one of those genres.\n",
    "\n",
    "### So - how do we improve on this?\n",
    "- Experiment with different numbers of features to look for. We started with 10, but you should expreriment toggling it up and down. Keep in mind that when you add tokens, even though it might make finding the genre easier, you also run the risk of catching other erroneous genres. This is a double edged sword.\n",
    "- Turn the above steps into a sklearn pipeline so that you can easily run a few dozen attempts, and so that the data scientist you're likely building this pipeline for can easily grid search on it.\n",
    "- Use a better vectorization method. There are lot's of better ways to do this that are more likely to be used in the real world:\n",
    "    - TF-IDF stands for Term Frequency - Inverse Document Frequency. This method lowers the value given to a word if it appears many times in the overall dataset. So for example, in our book descriptions we're looking at we're specifically looking for words that are common to *only* the Fantasy genre. Words that are generally common like 'book' or aren't particularly helpful.\n",
    "    \n",
    "    - Use something more advanced than word frequency based methods\n",
    "\n",
    "- Lemmatize the words. This is cutting the words down to just their stems so that words with similar roots are grouped together.\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
