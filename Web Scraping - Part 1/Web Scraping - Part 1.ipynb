{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.3.0-py3-none-any.whl (981 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.4/981.4 KB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting trio-websocket~=0.9\n",
      "  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
      "Collecting trio~=0.17\n",
      "  Downloading trio-0.21.0-py3-none-any.whl (358 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m359.0/359.0 KB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: urllib3[secure,socks]~=1.26 in /opt/homebrew/Cellar/jupyterlab/3.4.3/libexec/lib/python3.10/site-packages (from selenium) (1.26.9)\n",
      "Collecting sortedcontainers\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: idna in /opt/homebrew/Cellar/jupyterlab/3.4.3/libexec/lib/python3.10/site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Collecting outcome\n",
      "  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: sniffio in /opt/homebrew/Cellar/jupyterlab/3.4.3/libexec/lib/python3.10/site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/homebrew/Cellar/jupyterlab/3.4.3/libexec/lib/python3.10/site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Collecting async-generator>=1.9\n",
      "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
      "Collecting wsproto>=0.14\n",
      "  Downloading wsproto-1.1.0-py3-none-any.whl (24 kB)\n",
      "Collecting PySocks!=1.5.7,<2.0,>=1.5.6\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Collecting pyOpenSSL>=0.14\n",
      "  Downloading pyOpenSSL-22.0.0-py2.py3-none-any.whl (55 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 KB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cryptography>=1.3.4\n",
      "  Downloading cryptography-37.0.4-cp36-abi3-macosx_10_10_universal2.whl (5.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi in /opt/homebrew/Cellar/jupyterlab/3.4.3/libexec/lib/python3.10/site-packages (from urllib3[secure,socks]~=1.26->selenium) (2022.6.15)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/homebrew/Cellar/jupyterlab/3.4.3/libexec/lib/python3.10/site-packages (from cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (1.15.0)\n",
      "Collecting h11<1,>=0.9.0\n",
      "  Downloading h11-0.13.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.2/58.2 KB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pycparser in /opt/homebrew/Cellar/jupyterlab/3.4.3/libexec/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (2.21)\n",
      "Installing collected packages: sortedcontainers, PySocks, outcome, h11, async-generator, wsproto, trio, cryptography, trio-websocket, pyOpenSSL, selenium\n",
      "Successfully installed PySocks-1.7.1 async-generator-1.10 cryptography-37.0.4 h11-0.13.0 outcome-1.2.0 pyOpenSSL-22.0.0 selenium-4.3.0 sortedcontainers-2.4.0 trio-0.21.0 trio-websocket-0.9.2 wsproto-1.1.0\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.2.1 is available.\n",
      "You should consider upgrading via the '/opt/homebrew/Cellar/jupyterlab/3.4.3/libexec/bin/python3.10 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro To Webscraping\n",
    "\n",
    "We're going to learn how to scrape data from a website. It is quite common that you might want to scrape a bunch of data from a website in order to analyse it. For example:\n",
    "\n",
    " - Scrape a bunch of comments/social media posts to analyze sentiment\n",
    " - Scrape a bunch of data saved as .csv files somewhere\n",
    " - Pull public records that don't have a native download option on the webpage\n",
    " - etc, etc\n",
    " \n",
    "Webscraping is SUPER powerful, because it enables you to create datasets out of almost anything you can find online. We'll walk through a toy example here, and then you can feel free to identify the website of your choice and scrape away!\n",
    "\n",
    "We'll need some tools before we get started:\n",
    "\n",
    "I should point out, there are multiple frameworks to use for webscraping: beautifulsoup/selenium+chromedriver/requests/urlib are all fairly common and are used for different applications. Each scraping task will require slightly different capabilities, and require choosing the correct tooling. We'll focus on the first two here.\n",
    "\n",
    "- First, make sure you have Chrome. You should already, as it's the best browser on the planet :)\n",
    "\n",
    "- Second, download and run the appropriate: <a href=\"https://chromedriver.chromium.org/downloads\">ChromeDriver</a>\n",
    "\n",
    "- Third, make sure you have both selenium and beautifulsoup4\n",
    "\n",
    "\n",
    "## -----------DISCLAIMER!!!-------------WARNING!!!--------------\n",
    "\n",
    "Many websites prohibit webscraping. This is not to say people don't do it all the time anyways, but we'll need to play by the rules here. If you search online, there are many awesome uses/examples/tutorials on webscraping. One common excercise is to scrape Indeed or LinkedIn for job postings in a given city, to figure out the most in demand skills. The problem is, many of the job boards prohibit scraping. So, just for liabilities sake, we're going to do a toy example here. <a href=\"http://toscrape.com\">Toscrape.com</a> is a free website that was specifically set up for scraping, so we can play around with it without worrying about a specific site's Terms of Service changing on us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import selenium and beautifulsoup\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rw/1rnjc7b90vd_mr3753j04_s40000gp/T/ipykernel_53085/2996668081.py:3: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome('/Users/kendra/Downloads/chromedriver-2')\n"
     ]
    }
   ],
   "source": [
    "#point selenium to your download of the chromedriver and instantiate a driver\n",
    "\n",
    "driver = webdriver.Chrome('/Users/kendra/Downloads/chromedriver-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test it out and see if it's working\n",
    "\n",
    "driver.get('http://books.toscrape.com/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ok - let's get to scraping. Let's build a dataset that has the following for each book:\n",
    "\n",
    "- Title\n",
    "- Price\n",
    "- Description\n",
    "- Rating\n",
    "- Genre\n",
    "\n",
    "We'll use the same general process for each of the above. Web scraping can involve a bit of trial and error. Essentially you're just trying to figure out:\n",
    "- Where does the element you're looking for live in the html?\n",
    "- What is it's tag or XPath?\n",
    "- What is the page structure? I.e, once you've found the tag you want, how can you loop through the whole page to find ALL of the thing you're interested in (like all the titles).\n",
    "\n",
    "## I'll give an example below, then you can replicate it for each additional element you need.\n",
    "\n",
    " - right click one of the titles on the page and click 'inspect'\n",
    " - look at the section of html that pops up and right click that section of html and hover over 'copy'\n",
    " - click 'copy XPath'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we'll use the find_elements_by_xpath function on the XPath we copied to see what we get:\n",
    "\n",
    "title_element = driver.find_element(\"xpath\",'//*[@id=\"default\"]/div/\\\n",
    "                                              div/div/div/section/div[2]/ol/li[1]/article/h3/a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A Light in the ...'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using '.text' on this element will show us the text displayed on the page\n",
    "\n",
    "title = title_element.text\n",
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A Light in the Attic'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#that seems truncated - let's look back at the html and see if there's a tag that holds the whole title\n",
    "#get_attribute will help here\n",
    "\n",
    "title = title_element.get_attribute('title')\n",
    "title\n",
    "\n",
    "#there we go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ok - now we've found where each title is stored within the html of the page!\n",
    "\n",
    "Now we need to repeat this process for every book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Magic! I've scraped 20 titles!\n"
     ]
    }
   ],
   "source": [
    "# lets write a loop that pulls every title for the page and saves it in a list\n",
    "\n",
    "title_list = []\n",
    "\n",
    "for i in range(1,21): #this range represents the number of books per page\n",
    "\n",
    "    title_element = driver.find_element(\"xpath\",'//*[@id=\"default\"]/div/div/div/div/section/\\\n",
    "                                                    div[2]/ol/li[{}]/article/h3/a'.format(i)) #notice the .format()\n",
    "    title_list.append(title_element.get_attribute('title'))\n",
    "    \n",
    "print('Magic! I\\'ve scraped {} titles!'.format(len(title_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Magic! I've scraped 1000 titles!\n"
     ]
    }
   ],
   "source": [
    "#the loop above only scraped the first page. let's nest another loop to scrape every page\n",
    "\n",
    "title_list = []\n",
    "\n",
    "for i in range(1,51):\n",
    "\n",
    "    driver.get('http://books.toscrape.com/catalogue/page-{}.html'.format(i))\n",
    "\n",
    "    for x in range(1,21): #this range represents the number of books per page\n",
    "\n",
    "        title_element = driver.find_element(\"xpath\",'//*[@id=\"default\"]/div/div/div/div/section/\\\n",
    "                                                        div[2]/ol/li[{}]/article/h3/a'.format(x))\n",
    "        title_list.append(title_element.get_attribute('title'))\n",
    "        \n",
    "#quit your session so you don't have any ghost browsing sessions running :)\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "print('Magic! I\\'ve scraped {} titles!'.format(len(title_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way we've done it above actually launches a Chrome GUI browser. This is computationally expensive, and therefore, bad practice. It's cool, because you can follow along and literally see the scraping happening. That said, in practice you'll likely want to do this in what's called a 'headless' manner. Go online and read up on how to run selenium without launching a GUI browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rw/1rnjc7b90vd_mr3753j04_s40000gp/T/ipykernel_53085/2372497546.py:7: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome('/Users/kendra/Downloads/chromedriver-2', options=options)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Magic! I've scraped 1000 titles!\n"
     ]
    }
   ],
   "source": [
    "#set the options for the driver such that Chrome doesn't actually launch a GUI\n",
    "\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "options = Options()\n",
    "options.headless = True\n",
    "driver = webdriver.Chrome('/Users/kendra/Downloads/chromedriver-2', options=options)\n",
    "\n",
    "\n",
    "title_list = []\n",
    "\n",
    "for i in range(1,51):\n",
    "\n",
    "    driver.get('http://books.toscrape.com/catalogue/page-{}.html'.format(i))\n",
    "\n",
    "    for x in range(1,21): #this range represents the number of books per page\n",
    "\n",
    "        title_element = driver.find_element(\"xpath\",'//*[@id=\"default\"]/div/div/div/div/section/\\\n",
    "                                                        div[2]/ol/li[{}]/article/h3/a'.format(x))\n",
    "        title_list.append(title_element.get_attribute('title'))\n",
    "        \n",
    "#quit your session so you don't have any ghost browsing sessions running :)\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "print('Magic! I\\'ve scraped {} titles!'.format(len(title_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There we go! We've scraped all 1000 titles. Let's step back for a moment now.\n",
    "\n",
    "Above we did a quick and dirty scrape of every title on the website. We kept it super high level, and never navigated to any individual books's page. In order to pull more data for each book, let's dive in deeper.\n",
    "\n",
    "In this next section we'll first try to figure out if there's a pattern we can follow for each page's link. Then we'll loop through every page and scrape as much as we can from it.\n",
    "\n",
    "The selenium/ChromeDriver toolset is really cool, and helps visually illustrate what a webscraper is doing. In the next lab, we'll use beautifulsoup to get into the weeds."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
